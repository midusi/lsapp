<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <link rel="icon" type="image/svg+xml" href="../static/images/favicon.svg" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.2.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha384-iYQeCzEYFbKjA/T2uDLTpkwGzCiq6soy8tYaI1GyVh/UjpbCx/TYkiZhlZB6+fzT" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap-icons@1.9.1/font/bootstrap-icons.css">
  <link rel="stylesheet" href="../static/styles/style.css" />
  <title>LSApp</title>
</head>
<body class="d-flex flex-column min-vh-100 justify-content-between">

  <script src="https://unpkg.com/vue@3"></script>
  <script type="module">
    import Footer from '../static/vue/Footer.js'
    import Header from '../static/vue/Header.js'
    const { createApp } = Vue
    createApp(Header).mount('#header')
    createApp(Footer).mount('#footer')
  </script>

  <div id="header"></div>

  <main>
  <!-- FAQ With Accordion -->
  <section id="questions" class="p-5">
    <div class="container">
      <h2 class="text-center mb-4">
        Preguntas Frecuentes
      </h2>
      <div class="accordion accordion-flush" id="accordionFlushExample">
        <div class="accordion-item">
          <h2 class="accordion-header" id="flush-headingOne">
            <button class="accordion-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#flush-collapseOne" aria-expanded="false" aria-controls="flush-collapseOne">
              ¿Cómo está entrenada la inteligencia artificial?
            </button>
          </h2>
          <div id="flush-collapseOne" class="accordion-collapse collapse" aria-labelledby="flush-headingOne" data-bs-parent="#accordionFlushExample">
            <div class="accordion-body">
              Mediante la base de datos LSA-T, la primera en permitir la traducción de señas continuas en LSA. Contiene 14.880 sentencias a nivel de videos en LSA extraídos del canal de YouTube <a href="https://www.youtube.com/c/CNSORDOSARGENTINA">CN Sordos</a>, junto con las anotaciones para sus respectivas etiquetas y el agregado de la detección de keypoints para cada señante. Esto hace que el modelo de PyTorch esté entrenado directamente de datos generados por personas sordas. Más información en <a href="https://midusi.github.io/LSA-T/">LSA-T: The first continuous LSA dataset</a>.
            </div>
          </div>
        </div>
        <div class="accordion-item">
          <h2 class="accordion-header" id="flush-headingTwo">
            <button class="accordion-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#flush-collapseTwo" aria-expanded="false" aria-controls="flush-collapseTwo">
              ¿Puedo contribuir al proyecto?
            </button>
          </h2>
          <div id="flush-collapseTwo" class="accordion-collapse collapse" aria-labelledby="flush-headingTwo" data-bs-parent="#accordionFlushExample">
            <div class="accordion-body">
              ¡Sí! Es Open-Source y se encuentra alojado en GitHub en los repositorios: <a href="https://github.com/midusi/LSA-T">gh/midusi/lsa-t</a>, <a href="https://github.com/midusi/keypoint-models">gh/midusi/keypoint-models</a> y <a href="https://github.com/midusi/lsapp">gh/midusi/lsapp</a>.
            </div>
          </div>
        </div>
        <div class="accordion-item">
          <h2 class="accordion-header" id="flush-headingThree">
            <button class="accordion-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#flush-collapseThree" aria-expanded="false" aria-controls="flush-collapseThree">
              ¿Cómo funciona?
            </button>
          </h2>
          <div id="flush-collapseThree" class="accordion-collapse collapse" aria-labelledby="flush-headingThree" data-bs-parent="#accordionFlushExample">
            <div class="accordion-body">
              <h1>Sistema de Reconocimiento de Lengua de Señas <span class="text-primary">Argentina</span> basado en Inteligencia Artificial</h1>

              <hr>
              <p class="lead">
                Autores: Dal Bianco, P., Ríos, G., Ronchetti, F., Quiroga, F., Stanchi, O., Hasperué, W., &amp; Rosete, A.
                <br>
                <small class="text-muted">Instituto de Investigacion en Informática - LIDI (Universidad Nacional de La Plata, 2022)</small>
              </p>
              <hr>

              <p>
                El Reconocimiento Automático de Gestos permite a una computadora comprender los movimientos y poses de una persona, capturadas tradicionalmente mediante cámaras de video y utilizando modelos de Visión por Computadora. El Reconocimiento de Lengua de Señas (LS) es un caso particular de este campo, cuyo objetivo final es traducir mensajes en dicha lengua al castellano u otra lengua tradicional. Un sistema de reconocimiento de Lengua de Señas (RLS) facilitaría la integración de la comunidad sorda mediante la traducción automática, pero también mediante el desarrollo de sistemas automatizados de enseñanza de LS, dada la falta de intérpretes y docentes de LS [Bra19].
              </p>

              <p>
                En particular, en los últimos tiempos Las Redes Neuronales Convolucionales (CNNs) han mostrado excelentes resultados en problemas de Visión por Computadora, y son el modelo más utilizado en problemas de clasificación de imágenes [Qui21]. Con los últimos avances de hardware, posibilitan así el desarrollo de aplicaciones de uso regular con características avanzadas.
              </p>

              <p>
                El objetivo general de este plan de trabajo es el desarrollo de una aplicación prototipo para el reconocimiento de lengua de señas argentina, utilizando modelos de inteligencia artificial basados en redes neuronales previamente desarrollados por el grupo de investigación.
              </p>

              <p>
                El Reconocimiento Automático de Señas, un subárea del Reconocimiento de Gestos, es un problema multidisciplinar sumamente complejo que hoy en día sigue sin ser resuelto en forma total. Si bien en el último tiempo han habido avances en el reconocimiento de gestos, impulsados principalmente por el desarrollo de nuevas tecnologías, aún queda un largo camino por recorrer para construir aplicaciones precisas y robustas que permitan la traducción e interpretación de los gestos realizados por un intérprete.
              </p>

              <p>
                La compleja naturaleza de los gestos motivan esfuerzos de diversas áreas de investigación como interacción hombre-máquina, visión por computador, análisis de movimientos, aprendizaje automático y reconocimiento de patrones [Bra19]. La difusión y enseñanza de la lengua de señas, y particularmente la Lengua de Señas Argentino (LSA), es una temática muy impulsada actualmente por gobiernos y universidades para incluir a personas de la comunidad sorda en diferentes entornos [Ron16b].
              </p>

              <p>
                Una particularidad de la lengua de señas es que cada región a nivel mundial tiene su propio léxico y grupo de señas que lo representan. Esto lo hace un problema diverso, y diferente de abordar en cada región, ya que la diversidad en la ejecución de las señas o su combinación implica que cada lengua de señas requiere su propia base de datos. En particular, para la Lengua de Señas Argentino (LSA) prácticamente no existen sistemas y bases de datos para la investigación, excepto las bases de datos LSA16, de formas de mano, y LSA64 de gestos dinámicos, ambas desarrolladas por este grupo de investigación en el III-LIDI.
              </p>

              <p>
                La tarea completa de reconocer un gesto de lengua de señas involucra diferentes pasos: la ubicación de las manos del intérprete, el reconocimiento de las formas de las manos (configuraciones), el seguimiento de las manos para detectar el movimiento realizado, la ubicación y seguimiento de la cara, el reconocimiento de expresiones faciales, la interpretación semántica y finalmente la traducción al lenguaje escrito [Bra19]. Debido a su complejidad, los enfoques para el reconocimiento de gestos y lenguas de seña utilizan en su mayoría técnicas de Visión por Computadora, que combinan Aprendizaje Automático con Procesamiento de Imágenes [Car18][Zhu18][Kol15][Qui21].
              </p>

              <p>
                El Aprendizaje Automático es una rama de la Inteligencia Artificial que estudia sistemas capaces de aprender a realizar una tarea a partir de datos de ejemplo. [Car18][Zhu18][Kol15]. En los últimos años el procesamiento de texto, sonido, video y otras señales ha experimentado grandes progresos mediante el uso de una técnica de Aprendizaje Automático denominada Redes Neuronales Profundas o Aprendizaje Profundo (Deep Learning), que extiende los modelos previos de redes neuronales artificiales con arquitecturas y algoritmos de optimización que permiten entrenar redes con millones de parámetros, en base grandes cantidades de datos de entrenamiento [Car18][Zhu18][Kol15].
              </p>

              <p>
                Las redes convolucionales profundas (CNNs) son modelos del estado del arte que combinan elementos de las redes neuronales con convoluciones cuyos filtros son aprendidos a partir de los datos. Su aplicación principal es en tareas de Aprendizaje Automático con imágenes y videos  [Cam17][Yan17b][Mas17].
              </p>

              <p>
                Existen diversos trabajos en la actualidad que utilizan estos tipos de redes para clasificar lengua de señas y con un buen nivel de desempeño [Kol19][Qui21]. Además, en años recientes, los modelos basados en redes han alcanzado un nivel de desempeño aceptable para el desarrollo de aplicaciones para usuarios finales [Kol19][Qui21]. Por ende, es un momento propicio para el desarrollo de un sistema de reconocimiento de señas que sea realmente usable [Kol21].
              </p>

              <p>
                Un sistema de reconocimiento automático de lengua de señas argentina tiene varias aplicaciones posibles. En principio, permitiría a las personas sordas utilizar su lengua natural y traducirla a la lengua escrita para comunicarse con otras personas. En particular, permitiría subtitular automáticamente videos, por ejemplo, que suban o envíen personas sordas por internet a otras personas, de modo que puedan comunicarse de igual forma con personas que sepan lengua de señas y aquellas que no, dado que estas últimas representan la gran mayoría de la  población argentina y del mundo [Bra19].
              </p>

              <p>
                Finalmente, dado que la disponibilidad de intérpretes y docentes de lengua de señas es limitada, también permite desarrollar un sistema adjunto que permita una retroalimentación a quién se encuentra en proceso de aprendizaje de la lengua de señas argentina.
              </p>

              <p>
                <ul>
                  <li><a href="https://paperswithcode.com/dataset/lsa-t" target="_blank"><strong>LSA-T Dataset | Papers With Code</strong></a></li>
                  <li><a href="https://arxiv.org/abs/2211.15481" target="_blank"><strong>[2211.15481] LSA-T: The first continuous Argentinian Sign Language dataset for Sign Language Translation</strong></a></li>
                </ul>
              </p>
            </div>
          </div>
        </div>
        <div class="accordion-item">
          <h2 class="accordion-header" id="flush-headingFour">
            <button class="accordion-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#flush-collapseFour" aria-expanded="false" aria-controls="flush-collapseFour">
              ¿Todos las LS son iguales?
            </button>
          </h2>
          <div id="flush-collapseFour" class="accordion-collapse collapse" aria-labelledby="flush-headingFour" data-bs-parent="#accordionFlushExample">
            <div class="accordion-body">
              No, una particularidad de la lengua de señas es que cada región a nivel mundial tiene su propio léxico y grupo de señas que lo representan.
            </div>
          </div>
        </div>
        <div class="accordion-item">
          <h2 class="accordion-header" id="flush-headingFive">
            <button class="accordion-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#flush-collapseFive" aria-expanded="false" aria-controls="flush-collapseFive">
              ¿Quién puede acceder a los videos que subo a la aplicación?
            </button>
          </h2>
          <div id="flush-collapseFive" class="accordion-collapse collapse" aria-labelledby="flush-headingFive" data-bs-parent="#accordionFlushExample">
            <div class="accordion-body">
              ¡Nadie! Ni siquiera nosotros, los videos al ser grabados se procesan en tu mismo navegador, sin siquiera la necesidad de que sean guardados temporalmente en un servidor externo. Al servidor solo se envían los <i>'keypoints'</i> para el procesamiento de la estimación de poses que realizó el señante.
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>
  </main>

  <div id="footer"></div>

  <script>
    document.addEventListener("DOMContentLoaded", function(event) {
      var url = window.location.href;
      var start = url.substr(url.indexOf("#"));
      if (start[0] == "#") {
        new bootstrap.Collapse(
          document.querySelector(start), {togle: false}).show();
      }
    });
  </script>
  <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.2.1/dist/js/bootstrap.bundle.min.js" integrity="sha384-u1OknCvxWvY5kfmNBILK2hRnQC3Pr17a+RTT6rIHI7NnikvbZlHgTPOOmMi466C8" crossorigin="anonymous"></script>
</body>
</html>